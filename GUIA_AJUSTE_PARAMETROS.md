# üéõÔ∏è Gu√≠a de Ajuste de Par√°metros BERTopic

Esta gu√≠a explica c√≥mo ajustar los par√°metros del an√°lisis seg√∫n los resultados obtenidos.

---

## üìä Diagn√≥stico: ¬øQu√© problemas tengo?

### Problema 1: **Demasiados t√≥picos peque√±os** (muchos t√≥picos con pocos documentos)

**S√≠ntomas:**
- 15+ t√≥picos
- Muchos t√≥picos con solo 2-5 documentos
- T√≥picos muy similares entre s√≠

**Soluci√≥n:**

```python
# En create_bertopic_model()

# 1Ô∏è‚É£ Aumentar tama√±o m√≠nimo de clusters
hdbscan_model = hdbscan.HDBSCAN(
    min_cluster_size=10,     # ‚¨ÜÔ∏è Subir de 6 a 10
    min_samples=5,           # ‚¨ÜÔ∏è Subir de 3 a 5
    ...
)

# 2Ô∏è‚É£ Aumentar vecinos en UMAP
umap_model = UMAP(
    n_neighbors=15,          # ‚¨ÜÔ∏è Subir de 12 a 15
    ...
)

# 3Ô∏è‚É£ Reducir m√°s agresivamente
# En analizar_documentos()
topic_model_reduced = topic_model.reduce_topics(
    docs_clean,
    nr_topics=6              # ‚¨áÔ∏è Bajar de 10 a 6-8
)
```

---

### Problema 2: **Muchos documentos sin t√≥pico** (t√≥pico -1)

**S√≠ntomas:**
- >30% de documentos con t√≥pico -1
- Documentos relevantes marcados como outliers

**Soluci√≥n:**

```python
# En create_bertopic_model()

# 1Ô∏è‚É£ Clusters m√°s peque√±os
hdbscan_model = hdbscan.HDBSCAN(
    min_cluster_size=4,      # ‚¨áÔ∏è Bajar de 6 a 4
    min_samples=2,           # ‚¨áÔ∏è Bajar de 3 a 2
    ...
)

# 2Ô∏è‚É£ UMAP m√°s flexible
umap_model = UMAP(
    n_neighbors=8,           # ‚¨áÔ∏è Bajar de 12 a 8
    min_dist=0.1,            # ‚¨ÜÔ∏è Aumentar de 0.0 a 0.1
    ...
)
```

---

### Problema 3: **Palabras poco interpretables** (palabras raras o sin sentido)

**S√≠ntomas:**
- Palabras muy raras dominan los t√≥picos
- Palabras gen√©ricas sin significado espec√≠fico
- Fragmentos de palabras

**Soluci√≥n:**

```python
# En create_bertopic_model()

# 1Ô∏è‚É£ Agregar m√°s stopwords espec√≠ficas del dominio
spanish_stopwords = [
    # ... stopwords base ...
    # Agregar palabras del contexto parlamentario:
    "se√±or", "se√±ora", "se√±or√≠a", "honorable",
    "diputado", "diputada", "presidente", "presidenta",
    "c√°mara", "congreso", "comisi√≥n", "proyecto",
    "art√≠culo", "inciso", "ley", "norma",
]

# 2Ô∏è‚É£ Filtrar palabras m√°s raras
vectorizer_model = CountVectorizer(
    stop_words=spanish_stopwords,
    ngram_range=(1, 2),
    min_df=3,                # ‚¨ÜÔ∏è Subir de 2 a 3 (palabra aparece en 3+ docs)
    max_df=0.6               # ‚¨áÔ∏è Bajar de 0.7 a 0.6 (m√°ximo 60% docs)
)
```

---

### Problema 4: **T√≥picos demasiado amplios** (poco espec√≠ficos)

**S√≠ntomas:**
- 3-5 t√≥picos gigantes
- Documentos muy diferentes en el mismo t√≥pico
- Palabras muy gen√©ricas

**Soluci√≥n:**

```python
# En create_bertopic_model()

# 1Ô∏è‚É£ Clusters m√°s peque√±os
hdbscan_model = hdbscan.HDBSCAN(
    min_cluster_size=4,      # ‚¨áÔ∏è Bajar de 6 a 4
    ...
)

# 2Ô∏è‚É£ Menos reducci√≥n de t√≥picos
# En analizar_documentos()
topic_model_reduced = topic_model.reduce_topics(
    docs_clean,
    nr_topics=15             # ‚¨ÜÔ∏è Subir de 10 a 15
)

# 3Ô∏è‚É£ UMAP m√°s detallado
umap_model = UMAP(
    n_neighbors=8,           # ‚¨áÔ∏è Bajar de 12 a 8
    n_components=10,         # ‚¨ÜÔ∏è Subir de 5 a 10
    ...
)
```

---

## üéØ Configuraciones Recomendadas por Tama√±o de Corpus

### Corpus muy peque√±o (50-100 documentos)

```python
# UMAP
umap_model = UMAP(
    n_neighbors=8,
    n_components=5,
    min_dist=0.0,
    metric="cosine",
    random_state=42
)

# HDBSCAN
hdbscan_model = hdbscan.HDBSCAN(
    min_cluster_size=4,
    min_samples=2,
    metric="euclidean",
    cluster_selection_method="eom",
    prediction_data=True
)

# Reducci√≥n final
nr_topics=6
```

### Corpus mediano (100-300 documentos)

```python
# UMAP
umap_model = UMAP(
    n_neighbors=12,
    n_components=5,
    min_dist=0.0,
    metric="cosine",
    random_state=42
)

# HDBSCAN
hdbscan_model = hdbscan.HDBSCAN(
    min_cluster_size=6,
    min_samples=3,
    metric="euclidean",
    cluster_selection_method="eom",
    prediction_data=True
)

# Reducci√≥n final
nr_topics=10
```

### Corpus grande (300+ documentos)

```python
# UMAP
umap_model = UMAP(
    n_neighbors=15,
    n_components=10,
    min_dist=0.0,
    metric="cosine",
    random_state=42
)

# HDBSCAN
hdbscan_model = hdbscan.HDBSCAN(
    min_cluster_size=10,
    min_samples=5,
    metric="euclidean",
    cluster_selection_method="eom",
    prediction_data=True
)

# Reducci√≥n final
nr_topics=15
```

---

## üîÑ Proceso Iterativo Recomendado

### Iteraci√≥n 1: Configuraci√≥n inicial (conservadora)

```python
min_cluster_size=6
min_samples=3
n_neighbors=12
nr_topics=10
```

**‚Üí Ejecutar y revisar resultados**

---

### Iteraci√≥n 2: Ajustar seg√∫n diagn√≥stico

**Si muchos outliers (-1):**
```python
min_cluster_size=4  # ‚¨áÔ∏è
min_samples=2       # ‚¨áÔ∏è
```

**Si muchos t√≥picos peque√±os:**
```python
min_cluster_size=8  # ‚¨ÜÔ∏è
nr_topics=6         # ‚¨áÔ∏è
```

**‚Üí Ejecutar y revisar resultados**

---

### Iteraci√≥n 3: Refinamiento fino

**Ajustar stopwords + min_df seg√∫n palabras observadas**

**‚Üí Ejecutar y revisar resultados finales**

---

## üìù Checklist de Calidad

Usa esta lista para evaluar si tus resultados son buenos:

- [ ] **N√∫mero de t√≥picos razonable** (5-12 para ~100 docs)
- [ ] **<30% outliers** (t√≥pico -1)
- [ ] **Palabras clave interpretables** (tienen sentido en el dominio)
- [ ] **Documentos coherentes** dentro de cada t√≥pico
- [ ] **T√≥picos distintos** entre s√≠ (no redundantes)
- [ ] **Balance** (no un t√≥pico gigante y muchos mini-t√≥picos)

---

## üéì Par√°metros Explicados

### HDBSCAN

| Par√°metro | Qu√© hace | Cu√°ndo subirlo | Cu√°ndo bajarlo |
|-----------|----------|----------------|----------------|
| `min_cluster_size` | M√≠nimo de docs por t√≥pico | Muchos t√≥picos peque√±os | Muchos outliers |
| `min_samples` | Densidad m√≠nima requerida | Ruido en t√≥picos | Muchos outliers |

### UMAP

| Par√°metro | Qu√© hace | Cu√°ndo subirlo | Cu√°ndo bajarlo |
|-----------|----------|----------------|----------------|
| `n_neighbors` | Contexto local vs global | T√≥picos muy granulares | T√≥picos muy amplios |
| `n_components` | Dimensiones finales | Corpus grande (300+) | Corpus peque√±o (<100) |
| `min_dist` | Separaci√≥n entre puntos | Muchos outliers | T√≥picos se mezclan |

### Vectorizer

| Par√°metro | Qu√© hace | Cu√°ndo subirlo | Cu√°ndo bajarlo |
|-----------|----------|----------------|----------------|
| `min_df` | Frecuencia m√≠nima | Palabras raras | Vocabulario muy limitado |
| `max_df` | Frecuencia m√°xima | Palabras demasiado comunes | Pierdes t√©rminos importantes |
| `ngram_range` | Longitud de frases | Frases importantes | Vocabulario explota |

---

## üí° Tips Finales

1. **Cambiar UN par√°metro a la vez** para entender su efecto
2. **Guardar cada versi√≥n** con nombre descriptivo (ej: `resultados_mincluster8.xlsx`)
3. **Documentar cambios** en un archivo de notas
4. **Comparar visualmente** los resultados entre iteraciones
5. **Validar manualmente** revisando documentos de cada t√≥pico

---

¬°Buena suerte con el ajuste! üöÄ
